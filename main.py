"""
NACA 0012 Flutter PINN - ë©”ì¸ ì‹¤í–‰ íŒŒì¼
Physics-Informed Neural Networks for 2-DOF Flutter Analysis

ì „ì²´ PINN í•™ìŠµ íŒŒì´í”„ë¼ì¸:
1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
2. ëª¨ë¸ ì´ˆê¸°í™”
3. ìƒ˜í”Œë§ ì „ëµ ì„¤ì •
4. í•™ìŠµ ë£¨í”„ ì‹¤í–‰
5. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
"""

import torch
import torch.optim as optim
import numpy as np
import os
import argparse
from typing import Dict, Tuple, List, Optional

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤
from config import (
    PhysicalParameters, DomainParameters, PINNConfig, 
    TrainingConfig, FileConfig, parse_args, create_config_from_args
)
from data_io import DataProcessor, load_and_process_data
from pinn_model import PINNModel, create_pinn_model, save_model
from loss_functions import CompositeLoss
from boundary_conditions import BoundaryConditionManager, create_boundary_manager
from structure_dynamics import TwoDOFStructure, AerodynamicLoads, create_structure_system
from samplers import CompositeSampler, create_composite_sampler
from utils import (
    initialize_utils, Timer, system_monitor, loss_tracker,
    get_device, print_model_summary, create_progress_bar,
    FlowFieldVisualizer, ResultsManager
)

class PINNTrainer:
    """PINN í•™ìŠµ ê´€ë¦¬ì"""
    
    def __init__(self, phys_params: PhysicalParameters,
                 domain_params: DomainParameters,
                 pinn_config: PINNConfig,
                 training_config: TrainingConfig,
                 file_config: FileConfig,
                 coord_sys: str = "lab"):
        
        self.phys_params = phys_params
        self.domain_params = domain_params
        self.pinn_config = pinn_config
        self.training_config = training_config
        self.file_config = file_config
        self.coord_sys = coord_sys
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = get_device()
        
        # ê²°ê³¼ ê´€ë¦¬ì
        self.results_manager = ResultsManager(file_config.output_dir)
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self._initialize_components()
        
    def _initialize_components(self):
        """êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        from loguru import logger
        from pathlib import Path
        
        # ì¶œë ¥ í´ë” ìƒì„± (Windows ì•ˆì „)
        output_path = Path(self.file_config.output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        logger.info("ğŸ”§ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. ë°ì´í„° ì²˜ë¦¬ê¸°
        self.data_processor = DataProcessor(
            self.phys_params, self.domain_params, self.file_config
        )
        
        # 2. ê²½ê³„ì¡°ê±´ ê´€ë¦¬ì
        self.bc_manager = create_boundary_manager(
            self.phys_params, self.domain_params, self.coord_sys
        )
        
        # 3. êµ¬ì¡° ì‹œìŠ¤í…œ
        self.structure = create_structure_system(self.phys_params)
        
        # 4. ê³µë ¥ í•˜ì¤‘ ê³„ì‚°ê¸°
        self.aero_loads = AerodynamicLoads(self.phys_params)
        
        # 5. ìƒ˜í”ŒëŸ¬
        self.sampler = create_composite_sampler(
            self.phys_params, self.domain_params, self.training_config
        )
        
        # 6. PINN ëª¨ë¸
        self.model = create_pinn_model(self.pinn_config).to(self.device)
        print_model_summary(self.model)
        
        # 7. ì†ì‹¤ í•¨ìˆ˜
        domain_bounds = self.domain_params.get_nondim_bounds(self.phys_params.C_phys)
        self.loss_function = CompositeLoss(
            self.phys_params, self.training_config, domain_bounds, self.coord_sys
        )
        
        # 8. ì‹œê°í™” ë„êµ¬
        self.visualizer = FlowFieldVisualizer(domain_bounds)
        
        logger.info("âœ… êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def prepare_data(self) -> Dict:
        """ë°ì´í„° ì¤€ë¹„"""
        from loguru import logger
        
        with Timer("ë°ì´í„° ì¤€ë¹„"):
            # CSV ë°ì´í„° ë¡œë“œ
            processor, processed_data = load_and_process_data(
                self.phys_params, self.domain_params, self.file_config
            )
            
            # ë°ì´í„° ë¶„í¬ ì‹œê°í™”
            processor.visualize_data_distribution(
                os.path.join(self.file_config.output_dir, "data_distribution.png")
            )
            
            logger.info("ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
            return processed_data
    
    def generate_training_samples(self) -> Dict[str, torch.Tensor]:
        """í•™ìŠµìš© ìƒ˜í”Œ ìƒì„±"""
        from loguru import logger
        
        with Timer("ìƒ˜í”Œë§"):
            samples = self.sampler.generate_training_samples(
                n_collocation=self.training_config.batch_size,
                n_boundary=200,
                n_surface=150,
                strategy="stratified"
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            for key in samples:
                if isinstance(samples[key], torch.Tensor):
                    samples[key] = samples[key].to(self.device)
                elif isinstance(samples[key], dict):
                    for subkey in samples[key]:
                        if isinstance(samples[key][subkey], torch.Tensor):
                            samples[key][subkey] = samples[key][subkey].to(self.device)
            
            # ìƒ˜í”Œ ë¶„í¬ ì‹œê°í™” (ì„ì‹œ ë¹„í™œì„±í™”)
            try:
                from pathlib import Path
                save_path = Path(self.file_config.output_dir) / "sampling_distribution.png"
                self.sampler.visualize_samples(
                    samples, save_path=str(save_path)
                )
            except Exception as e:
                logger.warning(f"âš ï¸ ì‹œê°í™” ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")
            
            logger.info("Sampling completed")
            return samples
    
    def compute_physics_residuals(self, x_collocation: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ë¬¼ë¦¬ ë²•ì¹™ ì”ì°¨ ê³„ì‚°"""
        x_collocation.requires_grad_(True)
        
        # ëª¨ë¸ ì˜ˆì¸¡
        output = self.model(x_collocation)
        
        # ê¸°ìš¸ê¸° ê³„ì‚°
        gradients = self.model.compute_gradients(x_collocation, output)
        
        return output, gradients
    
    def compute_boundary_residuals(self, boundary_points: Dict[str, torch.Tensor],
                                 structure_state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """ê²½ê³„ì¡°ê±´ ì”ì°¨ ê³„ì‚°"""
        bc_residuals = {}
        
        for bc_type, points in boundary_points.items():
            if len(points) > 0:
                points.requires_grad_(True)
                predictions = self.model(points)
                
                residual = self.bc_manager.apply_boundary_conditions(
                    points, predictions, bc_type, structure_state
                )
                bc_residuals[bc_type] = residual
        
        return bc_residuals
    
    def update_structure_state(self, t: float, damping_data: Dict) -> Dict[str, torch.Tensor]:
        """êµ¬ì¡° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        # ëŒí•‘ ë°ì´í„°ì—ì„œ í˜„ì¬ ì‹œê°„ì˜ ìƒíƒœ ë³´ê°„
        time_data = damping_data['time']
        
        # í˜„ì¬ ì‹œê°„ì— ê°€ì¥ ê°€ê¹Œìš´ ì¸ë±ìŠ¤ ì°¾ê¸°
        time_diff = np.abs(time_data - t)
        closest_idx = np.argmin(time_diff)
        
        structure_state = {
            'time': torch.tensor([t], device=self.device, requires_grad=True),
            'h': torch.tensor([damping_data['h'][closest_idx]], device=self.device, requires_grad=True),
            'theta': torch.tensor([damping_data['theta'][closest_idx]], device=self.device, requires_grad=True),
            'h_vel': torch.tensor([damping_data['h_vel'][closest_idx]], device=self.device, requires_grad=True),
            'theta_vel': torch.tensor([damping_data['theta_vel'][closest_idx]], device=self.device, requires_grad=True),
            'lift': torch.tensor([damping_data['Lift'][closest_idx]], device=self.device, requires_grad=True),
            'moment': torch.tensor([damping_data['Moment'][closest_idx]], device=self.device, requires_grad=True)
        }
        
        return structure_state
    
    def training_step(self, samples: Dict[str, torch.Tensor], 
                     processed_data: Dict, epoch: int) -> Dict[str, torch.Tensor]:
        """ë‹¨ì¼ í•™ìŠµ ìŠ¤í…"""
        
        # í˜„ì¬ ì‹œê°„ (ì—í¬í¬ ê¸°ë°˜ìœ¼ë¡œ ìˆœí™˜)
        time_range = self.domain_params.t_end - self.domain_params.t_start
        current_time = (epoch % 100) / 100.0 * time_range + self.domain_params.t_start
        
        # êµ¬ì¡° ìƒíƒœ ì—…ë°ì´íŠ¸ - LBFGSìš© gradient ë³´ì¥
        structure_state = self.update_structure_state(current_time, processed_data['damping_nd'])
        
        # ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ requires_grad ì¬ì„¤ì • (LBFGS ì•ˆì •ì„±)
        for key, value in samples.items():
            if isinstance(value, torch.Tensor):
                value.requires_grad_(True)
            elif isinstance(value, dict):
                for subkey, subvalue in value.items():
                    if isinstance(subvalue, torch.Tensor):
                        subvalue.requires_grad_(True)
        
        # 1. PDE ì”ì°¨ ê³„ì‚°
        output, gradients = self.compute_physics_residuals(samples['collocation'])
        
        # 2. ê²½ê³„ì¡°ê±´ ì”ì°¨ ê³„ì‚°
        boundary_dict = samples['boundary'].copy()
        boundary_dict['airfoil'] = samples['surface']  # surfaceë¥¼ airfoil boundaryë¡œ ì¶”ê°€
        bc_residuals = self.compute_boundary_residuals(boundary_dict, structure_state)
        
        # 3. ì´ ì†ì‹¤ ê³„ì‚°
        losses = self.loss_function.compute_total_loss(
            model=self.model,
            model_output=output,
            x_collocation=samples['collocation'],
            x_boundary=samples['boundary'],
            x_surface=samples['surface'],
            structure_data=structure_state,
            cfd_data={'bc_residuals': bc_residuals},
            gradients=gradients
        )
        
        return losses
    
    def train(self, processed_data: Dict):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
        from loguru import logger
        
        logger.info("PINN í•™ìŠµ ì‹œì‘")
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        optimizer_adam = optim.AdamW(
            self.model.parameters(), 
            lr=self.training_config.adam_lr,
            weight_decay=1e-6
        )
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer_adam, mode='min', factor=0.8, patience=200
        )
        
        # AMP ì„¤ì •
        scaler = torch.cuda.amp.GradScaler() if self.training_config.use_amp else None
        
        # Adam í•™ìŠµ
        logger.info(f"Phase 1: Adam ìµœì í™” ({self.training_config.adam_epochs} epochs)")
        self._train_phase(optimizer_adam, self.training_config.adam_epochs, 
                         processed_data, scaler, scheduler, "Adam")
        
        # LBFGS í•™ìŠµ (gradient ì˜¤ë¥˜ ë°©ì§€)
        logger.info(f"Phase 2: LBFGS ìµœì í™” ({self.training_config.lbfgs_epochs} epochs)")
        try:
            optimizer_lbfgs = optim.LBFGS(
                self.model.parameters(),
                lr=0.1,
                max_iter=20,
                max_eval=None,
                tolerance_grad=1e-7,
                tolerance_change=1e-9
            )
            self._train_phase(optimizer_lbfgs, self.training_config.lbfgs_epochs,
                             processed_data, None, None, "LBFGS")
        except RuntimeError as e:
            if "does not require grad" in str(e):
                logger.warning("âš ï¸ LBFGS gradient ì˜¤ë¥˜ ë°œìƒ - Adam ê²°ê³¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤")
                logger.warning(f"ì˜¤ë¥˜ ë‚´ìš©: {e}")
            else:
                raise e
        
        logger.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    
    def _train_phase(self, optimizer, num_epochs: int, processed_data: Dict,
                    scaler: Optional[torch.cuda.amp.GradScaler], 
                    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],
                    phase_name: str):
        """í•™ìŠµ í˜ì´ì¦ˆ ì‹¤í–‰"""
        from loguru import logger
        
        progress_bar = create_progress_bar(num_epochs, f"{phase_name} Training")
        
        for epoch in range(num_epochs):
            system_monitor.update()
            
            # ìƒˆë¡œìš´ ìƒ˜í”Œ ìƒì„± (ë§¤ ì—í¬í¬ë§ˆë‹¤)
            samples = self.generate_training_samples()
            
            def closure():
                optimizer.zero_grad()
                
                if scaler and phase_name == "Adam":
                    with torch.cuda.amp.autocast():
                        losses = self.training_step(samples, processed_data, epoch)
                    scaler.scale(losses['total']).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    losses = self.training_step(samples, processed_data, epoch)
                    losses['total'].backward()
                    if phase_name == "Adam":
                        optimizer.step()
                
                return losses['total']
            
            if phase_name == "LBFGS":
                optimizer.step(closure)
                # LBFGS: gradient ìœ ì§€í•œ ì±„ë¡œ loss ê³„ì‚°
                losses = self.training_step(samples, processed_data, epoch)
            else:
                total_loss = closure()
                losses = self.training_step(samples, processed_data, epoch)
            
            # ì†ì‹¤ ê¸°ë¡
            loss_dict = {k: v.item() if isinstance(v, torch.Tensor) else v 
                        for k, v in losses.items()}
            loss_tracker.update(loss_dict, epoch)
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            if scheduler and phase_name == "Adam":
                scheduler.step(losses['total'])
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'Total': f"{losses['total'].item():.2e}",
                'PDE': f"{losses['pde'].item():.2e}",
                'BC': f"{losses['bc'].item():.2e}",
                'FSI': f"{losses['fsi'].item():.2e}"
            })
            progress_bar.update(1)
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë§¤ 100 ì—í¬í¬)
            if (epoch + 1) % 100 == 0:
                self.results_manager.save_model_checkpoint(
                    self.model, optimizer, epoch, losses['total'].item(),
                    metadata={'phase': phase_name, 'lr': optimizer.param_groups[0]['lr']}
                )
        
        progress_bar.close()
    
    def evaluate(self, processed_data: Dict):
        """ëª¨ë¸ í‰ê°€"""
        from loguru import logger
        
        with Timer("ëª¨ë¸ í‰ê°€"):
            self.model.eval()
            
            with torch.no_grad():
                # í‰ê°€ìš© ê·¸ë¦¬ë“œ ìƒì„±
                eval_grid = self._create_evaluation_grid()
                
                # ìœ ë™ì¥ ì˜ˆì¸¡
                flow_predictions = self.model(eval_grid)
                
                # êµ¬ì¡° ì‘ë‹µ ë¶„ì„
                self._analyze_structural_response(processed_data)
                
                # ê²°ê³¼ ì‹œê°í™”
                self._visualize_results(eval_grid, flow_predictions, processed_data)
        
        logger.info("ğŸ“ˆ í‰ê°€ ì™„ë£Œ")
    
    def _create_evaluation_grid(self) -> torch.Tensor:
        """í‰ê°€ìš© ê·¸ë¦¬ë“œ ìƒì„±"""
        domain_bounds = self.domain_params.get_nondim_bounds(self.phys_params.C_phys)
        
        # ê³µê°„ ê·¸ë¦¬ë“œ
        nx, ny = 100, 80
        x = np.linspace(domain_bounds['x_min'], domain_bounds['x_max'], nx)
        y = np.linspace(domain_bounds['y_min'], domain_bounds['y_max'], ny)
        X, Y = np.meshgrid(x, y)
        
        # ì‹œê°„ (ì¤‘ê°„ê°’)
        t_eval = (self.domain_params.t_start + self.domain_params.t_end) / 2
        T = np.full_like(X, t_eval)
        
        # í…ì„œë¡œ ë³€í™˜
        grid_points = torch.tensor(
            np.stack([T.flatten(), X.flatten(), Y.flatten()], axis=1),
            dtype=torch.float32, device=self.device
        )
        
        return grid_points
    
    def _analyze_structural_response(self, processed_data: Dict):
        """êµ¬ì¡° ì‘ë‹µ ë¶„ì„"""
        from structure_dynamics import ResponseAnalysis
        
        analyzer = ResponseAnalysis()
        damping_data = processed_data['damping_nd']
        
        # FFT ë¶„ì„
        h_fft = analyzer.fft_analysis(damping_data['time'], damping_data['h'])
        theta_fft = analyzer.fft_analysis(damping_data['time'], damping_data['theta'])
        
        # ì£¼íŒŒìˆ˜ ë¶„ì„ ê²°ê³¼ ì €ì¥
        results_summary = {
            'h_dominant_freq': analyzer.identify_dominant_frequency(
                damping_data['time'], damping_data['h']
            ),
            'theta_dominant_freq': analyzer.identify_dominant_frequency(
                damping_data['time'], damping_data['theta']
            ),
            'h_rms': analyzer.compute_rms(damping_data['h']),
            'theta_rms': analyzer.compute_rms(damping_data['theta'])
        }
        
        self.results_manager.save_results_summary(results_summary)
        
        # ì‘ë‹µ ë¶„ì„ í”Œë¡¯
        analyzer.plot_response_analysis(
            damping_data['time'], damping_data['h'], damping_data['theta'],
            damping_data['Lift'], damping_data['Moment'],
            save_path=os.path.join(self.file_config.output_dir, "structural_response.png")
        )
    
    def _visualize_results(self, grid_points: torch.Tensor, 
                          predictions: torch.Tensor, processed_data: Dict):
        """ê²°ê³¼ ì‹œê°í™”"""
        
        # ê·¸ë¦¬ë“œ ì°¨ì› ë³µì›
        nx, ny = 100, 80
        
        # ì˜ˆì¸¡ê°’ì„ ê·¸ë¦¬ë“œë¡œ ì¬êµ¬ì„±
        u_pred = predictions[:, 0].cpu().numpy().reshape(ny, nx)
        v_pred = predictions[:, 1].cpu().numpy().reshape(ny, nx)
        p_pred = predictions[:, 2].cpu().numpy().reshape(ny, nx)
        
        x_grid = grid_points[:, 1].cpu().numpy().reshape(ny, nx)
        y_grid = grid_points[:, 2].cpu().numpy().reshape(ny, nx)
        
        # ì—ì–´í¬ì¼ ìœ¤ê³½ì„ 
        airfoil_x, airfoil_y = self.bc_manager.geometry.get_surface_points(100)
        
        # ìœ ë™ì¥ ì‹œê°í™”
        self.visualizer.plot_flow_field(
            x_grid, y_grid, u_pred, v_pred, p_pred,
            airfoil_x, airfoil_y,
            save_path=os.path.join(self.file_config.output_dir, "flow_field.png")
        )
        
        # ì†ì‹¤ ê³¡ì„  í”Œë¡¯
        loss_tracker.plot_curves(
            save_path=os.path.join(self.file_config.output_dir, "loss_curves.png")
        )
        
        # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ê²°ê³¼
        system_monitor.plot_history(
            save_path=os.path.join(self.file_config.output_dir, "system_monitoring.png")
        )

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
    args = parse_args()
    
    # ì„¤ì • ìƒì„±
    phys_params, domain_params, pinn_config, training_config, file_config = \
        create_config_from_args(args)
    
    # ìœ í‹¸ë¦¬í‹° ì´ˆê¸°í™”
    initialize_utils("INFO", file_config.output_dir)
    
    # í•™ìŠµê¸° ìƒì„±
    trainer = PINNTrainer(
        phys_params, domain_params, pinn_config, 
        training_config, file_config, args.coord_sys
    )
    
    try:
        # ë°ì´í„° ì¤€ë¹„
        processed_data = trainer.prepare_data()
        
        # í•™ìŠµ ì‹¤í–‰
        trainer.train(processed_data)
        
        # í‰ê°€ ë° ê²°ê³¼ ì €ì¥
        trainer.evaluate(processed_data)
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        save_model(trainer.model, file_config.model_save_path, {
            'training_config': training_config,
            'phys_params': phys_params,
            'final_loss': loss_tracker.best_loss
        })
        
        from loguru import logger
        logger.info(f"ğŸ¯ ìµœì¢… ëª¨ë¸ ì €ì¥: {file_config.model_save_path}")
        logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥: {loss_tracker.best_loss:.2e} (Epoch {loss_tracker.best_epoch})")
        
    except KeyboardInterrupt:
        from loguru import logger
        logger.warning("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # í˜„ì¬ ìƒíƒœ ì €ì¥
        save_model(trainer.model, file_config.model_save_path.replace('.pt', '_interrupted.pt'))
        
    except Exception as e:
        from loguru import logger
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise
    
    finally:
        # ì •ë¦¬ ì‘ì—…
        loss_tracker.save_history(
            os.path.join(file_config.output_dir, "loss_history.json")
        )

if __name__ == "__main__":
    main()